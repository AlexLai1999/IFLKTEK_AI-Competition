{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: seaborn in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.9.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from seaborn) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from seaborn) (1.3.0)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from seaborn) (0.24.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from matplotlib>=1.4.3->seaborn) (2.4.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from matplotlib>=1.4.3->seaborn) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from matplotlib>=1.4.3->seaborn) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from matplotlib>=1.4.3->seaborn) (2019.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (40.4.3)\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8-*-\r\n",
    "!pip install seaborn\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib as mpl\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\r\n",
    "import warnings\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\r\n",
    "import gc\r\n",
    "import time\r\n",
    "from datetime import timedelta, datetime\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data......\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "print('reading data......')\r\n",
    "trainFilepath = '/home/aistudio/data/data10020/round1_iflyad_anticheat_traindata.txt'\r\n",
    "testFilepath = '/home/aistudio/data/data10020/round1_iflyad_anticheat_testdata_feature.txt'\r\n",
    "df_train = pd.read_csv(trainFilepath, sep='\\t')\r\n",
    "df_test = pd.read_csv(testFilepath, sep='\\t')\r\n",
    "dataset = pd.concat([df_train, df_test], ignore_index=True)\r\n",
    "dataset['label'] = dataset['label'].fillna(-1).astype(int)\r\n",
    "del df_train, df_test\r\n",
    "gc.collect()\r\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各个变量缺失值情况：\n",
      " {'macmd5': 0, 'ip': 0, 'openudidmd5': 0, 'carrier': 0, 'reqrealip': 0, 'ver': 351016, 'osv': 13929, 'h': 0, 'make': 135343, 'dvctype': 0, 'ppi': 0, 'ntt': 0, 'os': 0, 'apptype': 0, 'sid': 0, 'mediashowid': 0, 'imeimd5': 0, 'province': 0, 'pkgname': 0, 'orientation': 0, 'label': 0, 'model': 4562, 'adidmd5': 0, 'w': 0, 'lan': 400862, 'city': 16660, 'idfamd5': 0, 'nginxtime': 0, 'adunitshowid': 0}\n"
     ]
    }
   ],
   "source": [
    "# NA in dataset\r\n",
    "col_na=dict()\r\n",
    "for col in dataset.keys():\r\n",
    "    col_na.setdefault(col,0)\r\n",
    "    col_na[col] = dataset[col].isna().sum()\r\n",
    "print('各个变量缺失值情况：\\n',col_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 填充object类型特征的NA\r\n",
    "object_col_name = dataset.dtypes[dataset.dtypes.values=='object'].keys().tolist()\r\n",
    "for object_col in object_col_name:\r\n",
    "    dataset[object_col] = dataset[object_col].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing......\n",
      "gaining time features......\n",
      "Finished in 8 seconds!\n"
     ]
    }
   ],
   "source": [
    "# 数据清洗与预处理\r\n",
    "print('data preprocessing......')\r\n",
    "# 'orientation'中有异常值 90 和 2 ，将之都归为0\r\n",
    "dataset.orientation[(dataset.orientation==2) | (dataset.orientation==90)] = 0\r\n",
    "\r\n",
    "# 'carrier'中异常值 -1 ，归为0\r\n",
    "dataset.carrier[dataset.carrier == -1] = 0\r\n",
    "\r\n",
    "# 'ntt' 处理，0、7均为未知 -> 0； 1、2宽带 -> 1； 3未知移动网 -> 2；4、5、6移动网 -> 3\r\n",
    "dataset.ntt[(dataset.ntt == 0) | (dataset.ntt == 7)] = 0\r\n",
    "dataset.ntt[(dataset.ntt == 1) | (dataset.ntt == 2)] = 1\r\n",
    "dataset.ntt[dataset.ntt == 3] = 2\r\n",
    "dataset.ntt[(dataset.ntt >= 4) & (dataset.ntt <= 6)] = 3\r\n",
    "# dataset.ntt[(dataset.carrier <= 0) | (dataset.carrier > 46003)]\r\n",
    "\r\n",
    "# 处理时间特征\r\n",
    "print('gaining time features......')\r\n",
    "start = time.time()\r\n",
    "dataset['datetime'] = pd.to_datetime(dataset['nginxtime']/1000,unit='s') + timedelta(hours=8)\r\n",
    "dataset['hour'] = dataset.datetime.dt.hour\r\n",
    "dataset['minute'] = dataset.datetime.dt.minute\r\n",
    "dataset['day'] = dataset.datetime.dt.day - dataset.datetime.dt.day.min()\r\n",
    "dataset['respond_time'] = abs(dataset['sid'].apply(lambda x : x.split('-')[-1]).astype(float)-dataset['nginxtime'])\r\n",
    "dataset = dataset.drop(columns = ['datetime','nginxtime'])\r\n",
    "print('Finished in %d seconds!' %(time.time()-start))\r\n",
    "del start\r\n",
    "# 处理 'make'\r\n",
    "def making(x):\r\n",
    "    x = x.lower()\r\n",
    "    if 'iphone' in x or 'apple' in x or '苹果' in x:\r\n",
    "        return 'iphone'\r\n",
    "    elif 'huawei' in x or 'honor' in x or '华为' in x or '荣耀' in x:\r\n",
    "        return 'huawei'\r\n",
    "    elif 'xiaomi' in x or '小米' in x or 'redmi' in x:\r\n",
    "        return 'xiaomi'\r\n",
    "    elif '魅族' in x:\r\n",
    "        return 'meizu'\r\n",
    "    elif '金立' in x:\r\n",
    "        return 'gionee'\r\n",
    "    elif '三星' in x or 'samsung' in x:\r\n",
    "        return 'samsung'\r\n",
    "    elif 'vivo' in x:\r\n",
    "        return 'vivo'\r\n",
    "    elif 'oppo' in x:\r\n",
    "        return 'oppo'\r\n",
    "    elif 'lenovo' in x or '联想' in x:\r\n",
    "        return 'lenovo'\r\n",
    "    elif 'nubia' in x:\r\n",
    "        return 'nubia'\r\n",
    "    elif 'oneplus' in x or '一加' in x:\r\n",
    "        return 'oneplus'\r\n",
    "    elif 'smartisan' in x or '锤子' in x:\r\n",
    "        return 'smartisan'\r\n",
    "    elif '360' in x or '360手机' in x:\r\n",
    "        return '360'\r\n",
    "    elif 'zte' in x or '中兴' in x:\r\n",
    "        return 'zte'\r\n",
    "    else:\r\n",
    "        return 'others'\r\n",
    "dataset['make'] = dataset['make'].astype(str).apply(lambda x : x.lower())\r\n",
    "dataset['make'] = dataset['make'].apply(making)\r\n",
    "dataset.os[dataset.make == 'iphone'] = 'ios'\r\n",
    "dataset.os[dataset.make != 'iphone'] = 'android'\r\n",
    "\r\n",
    "# 处理'lan’\r\n",
    "def lan(x):\r\n",
    "    x = x.lower()\r\n",
    "    if x in ['zh-cn','zh','cn','zh_cn','zh_cn_#hans','zh-']:\r\n",
    "        return 'zh-cn'\r\n",
    "    elif x in ['tw','zh-tw','zh_tw']:\r\n",
    "        return 'zh-tw'\r\n",
    "    elif 'en' in x:\r\n",
    "        return 'en'\r\n",
    "    elif 'hk' in x:\r\n",
    "        return 'zh-hk'\r\n",
    "    else:\r\n",
    "        return x\r\n",
    "dataset['lan'] = dataset['lan'].astype(str).apply(lambda x : x.lower())\r\n",
    "dataset['lan'] = dataset['lan'].apply(lan)\r\n",
    "\r\n",
    "# 粗略处理 'ver'\r\n",
    "def ver_trans(x):\r\n",
    "    x = str(x)\r\n",
    "    for i in range(0,30):\r\n",
    "        for j in range(0,10):\r\n",
    "            if '3.'+str(j)+'.'+str(i) in x or '30'+str(j)+str(i) in x:\r\n",
    "                return '3.'+str(j)+'.'+str(i)\r\n",
    "    if '521000' in x or '5.2.1' in x:\r\n",
    "        return '5.2.1'\r\n",
    "    else:\r\n",
    "        return x\r\n",
    "dataset['ver']=dataset.ver.apply(ver_trans).fillna('Unknown')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting ip number in different macmd5......\n",
      "Finished in 9 sec\n",
      "counting ip number in different imeimd5......\n",
      "Finished in 15 sec\n"
     ]
    }
   ],
   "source": [
    "# 统计同一设备下的ip数\r\n",
    "start = time.time()\r\n",
    "print('counting ip number in different macmd5......')\r\n",
    "temp1 = dataset.groupby(['macmd5','ip'])\r\n",
    "temp2 = temp1.size().reset_index(name='mac_ip').drop('ip',axis=1)\r\n",
    "temp3 = temp2.groupby(by='macmd5').count().reset_index()\r\n",
    "dataset = pd.merge(dataset,temp3,how='left',on='macmd5').fillna(0)\r\n",
    "print('Finished in %d sec' %(time.time()-start))\r\n",
    "del start,temp1,temp2,temp3\r\n",
    "dataset.mac_ip[dataset.macmd5=='empty'] = 0\r\n",
    "start = time.time()\r\n",
    "print('counting ip number in different imeimd5......')\r\n",
    "temp1 = dataset.groupby(['imeimd5','ip'])\r\n",
    "temp2 = temp1.size().reset_index(name='imei_ip').drop('ip',axis=1)\r\n",
    "temp3 = temp2.groupby(by='imeimd5').count().reset_index()\r\n",
    "dataset = pd.merge(dataset,temp3,how='left',on='imeimd5').fillna(0)\r\n",
    "print('Finished in %d sec' %(time.time()-start))\r\n",
    "del start,temp1,temp2,temp3\r\n",
    "dataset.imei_ip[dataset.imeimd5=='empty'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding......\n",
      "降维后数据方差解释率为： 0.9115884842818075\n"
     ]
    }
   ],
   "source": [
    "# One-Hot 编码特征\r\n",
    "onehot_col_name = ['mediashowid','osv','province','dvctype','apptype','os','make','lan','ntt','carrier','ppi','city']\r\n",
    "ohe = OneHotEncoder()\r\n",
    "print('One-Hot Encoding......')\r\n",
    "ohe_result = ohe.fit_transform(dataset[onehot_col_name])\r\n",
    "# 高维稀疏特征，通过TruncatedSVD方法降维\r\n",
    "tsvd = TruncatedSVD(n_components=150)\r\n",
    "decomposition_feature = tsvd.fit_transform(ohe_result)\r\n",
    "mm = MinMaxScaler()\r\n",
    "decomposition_feature = mm.fit_transform(decomposition_feature)\r\n",
    "print('降维后数据方差解释率为：',tsvd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TSVD0     TSVD1    TSVD10   TSVD100   TSVD101   TSVD102   TSVD103  \\\n",
      "0  0.730870  0.158168  0.579168  0.418900  0.218006  0.456457  0.402786   \n",
      "1  0.535318  0.677373  0.636917  0.391467  0.190527  0.456830  0.409331   \n",
      "2  0.912436  0.206114  0.552902  0.376713  0.235333  0.466022  0.399146   \n",
      "3  0.530898  0.744761  0.336806  0.393727  0.198086  0.466891  0.401612   \n",
      "4  0.855237  0.347230  0.287094  0.407842  0.210157  0.487930  0.395314   \n",
      "\n",
      "    TSVD104   TSVD105   TSVD106  ...    TSVD90    TSVD91    TSVD92    TSVD93  \\\n",
      "0  0.236495  0.412124  0.365799  ...  0.406208  0.483100  0.305829  0.440837   \n",
      "1  0.215957  0.407825  0.354410  ...  0.355379  0.476560  0.342578  0.459193   \n",
      "2  0.236696  0.414698  0.368558  ...  0.483426  0.483188  0.212300  0.342869   \n",
      "3  0.218846  0.411580  0.354502  ...  0.369387  0.478911  0.354852  0.439324   \n",
      "4  0.237139  0.412057  0.365827  ...  0.383559  0.486163  0.334982  0.468335   \n",
      "\n",
      "     TSVD94    TSVD95    TSVD96    TSVD97    TSVD98    TSVD99  \n",
      "0  0.388715  0.251834  0.466273  0.366019  0.325556  0.386865  \n",
      "1  0.318066  0.282815  0.390329  0.327709  0.331834  0.353978  \n",
      "2  0.435950  0.247759  0.391833  0.365925  0.317155  0.434724  \n",
      "3  0.311064  0.287628  0.414908  0.329929  0.287533  0.360569  \n",
      "4  0.368891  0.243944  0.408910  0.355156  0.322443  0.379099  \n",
      "\n",
      "[5 rows x 150 columns]\n"
     ]
    }
   ],
   "source": [
    "decom = dict()\n",
    "for i in range(decomposition_feature.shape[1]):\n",
    "    col_name = 'TSVD'+str(i)\n",
    "    decom.setdefault(col_name,[])\n",
    "    decom[col_name] = decom[col_name]+decomposition_feature[:,i].tolist()\n",
    "decom = pd.DataFrame(decom)\n",
    "del decomposition_feature\n",
    "print(decom.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkgname value counting......\n",
      "adunitshowid value counting......\n",
      "ip value counting......\n",
      "reqrealip value counting......\n",
      "adidmd5 value counting......\n",
      "imeimd5 value counting......\n",
      "idfamd5 value counting......\n",
      "macmd5 value counting......\n",
      "reqrealip value counting......\n",
      "ver value counting......\n",
      "model value counting......\n"
     ]
    }
   ],
   "source": [
    "# 统计特征值出现次数\n",
    "def get_counts_feature(data,col_name):\n",
    "    print(col_name,'value counting......')\n",
    "    temp = pd.DataFrame(data[col_name].value_counts().reset_index())\n",
    "    temp.columns = [col_name,'counts']\n",
    "    result = pd.merge(data,temp,how='left',on=col_name)['counts']\n",
    "    return result.fillna(0)\n",
    "counts_col = ['pkgname', 'adunitshowid', 'ip', 'reqrealip',\n",
    "                    'adidmd5', 'imeimd5', 'idfamd5', 'macmd5','reqrealip','ver','model']\n",
    "for col in counts_col:\n",
    "    dataset[col+'_counts'] = get_counts_feature(dataset,col_name=col)\n",
    "del col\n",
    "counts_col_name = list(map(lambda x : x+'_counts',counts_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LabelEncode\n",
    "def labelcoder(col,data):\n",
    "    lbe = LabelEncoder()\n",
    "    result = lbe.fit_transform(data[col])\n",
    "    result_name = col+'_LabelCode'\n",
    "    data[result_name] = result\n",
    "    return data\n",
    "label_code_col = ['ip','reqrealip','w','h']\n",
    "for i in label_code_col:\n",
    "    dataset = labelcoder(i, dataset)\n",
    "labelcode_col = list(map(lambda x : x+'_LabelCode',label_code_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label', 'openudidmd5', 'h', 'w', 'sid'}\n"
     ]
    }
   ],
   "source": [
    "numerical_col_name = ['hour','minute','orientation','day','respond_time','mac_ip','imei_ip']+counts_col_name+labelcode_col\n",
    "surplus_col_name= set(dataset.keys().tolist())-set(onehot_col_name)-set(numerical_col_name)-set(counts_col)\n",
    "print(surplus_col_name)\n",
    "x = pd.concat([dataset[numerical_col_name],decom],axis=1)\n",
    "y = dataset.label\n",
    "del decom\n",
    "data = pd.concat([pd.concat([x,y],axis=1),dataset.sid],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 特征选择后划分数据集\n",
    "x = data.drop(['sid','label'],axis=1)\n",
    "trainx = x[:1000000]\n",
    "predict_x = x[1000000:]\n",
    "def featureSelect(x_train,y_train,x_val,func=chi2,percentile=80):\n",
    "    model = SelectPercentile(func,percentile=percentile)\n",
    "    model.fit(x_train,y_train)\n",
    "    x_train = model.transform(x_train)\n",
    "    x_val = model.transform(x_val)\n",
    "    return x_train,x_val\n",
    "trainx,predict_x = featureSelect(trainx,data.label[data.label != -1].values,predict_x)\n",
    "x_train,x_val,y_train,y_val = train_test_split(trainx,data.label[data.label != -1].values,\n",
    "train_size=0.8,random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm\r\n",
    "from lightgbm import LGBMClassifier\r\n",
    "!pip install xgboost\r\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1_metric(labels, preds):\r\n",
    "    score = f1_score(labels, np.round(preds))\r\n",
    "    return 'f1', score, True\r\n",
    "\r\n",
    "def f1_xgb(preds,dtrain):\r\n",
    "    labels = dtrain.get_label()\r\n",
    "    score = f1_score(labels, np.round(preds))\r\n",
    "    return 'f1', -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 基于hyperopt进行参数调优\n",
    "!pip install hyperopt\n",
    "from hyperopt import fmin, tpe, hp, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LGB 参数调优\n",
    "# 定义参数空间\n",
    "space_lgb = {\"max_depth\": hp.randint(\"max_depth\", 15),\n",
    "         \"n_estimators\": hp.randint(\"n_estimators\", 5000),\n",
    "         \"learning_rate\": hp.uniform(\"learning_rate\", 0.001, 0.5),\n",
    "         \"subsample\": hp.uniform(\"subsample\", 0.3, 1),\n",
    "         \"colsample_bytree\":hp.uniform(\"colsample_bytree\", 0.3, 1),\n",
    "         \"num_leaves\": hp.randint(\"num_leaves\", 10),\n",
    "         \"subsample_freq\": hp.randint(\"subsample_freq\",5),\n",
    "         \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.1, 3),\n",
    "         \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.1, 3)\n",
    "         }\n",
    "def argsDict_tranform_lgb(argsDict, isPrint=False):\n",
    "    argsDict[\"max_depth\"] = argsDict[\"max_depth\"] + 6\n",
    "    argsDict[\"n_estimators\"] = argsDict['n_estimators'] + 300\n",
    "    argsDict[\"learning_rate\"] = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "    argsDict[\"num_leaves\"] = argsDict[\"num_leaves\"] * 8 + 20\n",
    "    if isPrint:\n",
    "        print(argsDict)\n",
    "    else:\n",
    "        pass\n",
    "    return argsDict\n",
    "\n",
    "def get_tranformer_score(tranformer):\n",
    "    model = tranformer\n",
    "    prediction = model.predict(x_val, num_iteration=model.best_iteration_)\n",
    "    return -1*f1_score(y_val,np.round(prediction))\n",
    "\n",
    "# 创建模型工场\n",
    "def lightgbm_factory(argsDict):\n",
    "    argsDict = argsDict_tranform_lgb(argsDict)\n",
    "    lgb = LGBMClassifier(max_depth=argsDict['max_depth'],n_estimators=argsDict['n_estimators'],\n",
    "                        learning_rate=argsDict['learning_rate'],subsample=argsDict['subsample'],\n",
    "                        num_leaves=argsDict['num_leaves'],objective='binary',\n",
    "                        colsample_bytree=argsDict['colsample_bytree'],reg_alpha=argsDict['reg_alpha'],\n",
    "                        reg_lambda=argsDict['reg_lambda'],random_seed=66)\n",
    "    lgb.fit(x_train,y_train,\n",
    "    eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    eval_names=['train', 'val'],\n",
    "    eval_metric=f1_metric,\n",
    "    verbose = 50,\n",
    "    early_stopping_rounds=50\n",
    "    )\n",
    "    return get_tranformer_score(lgb)\n",
    "    \n",
    "algo = partial(tpe.suggest, n_startup_jobs=1)\n",
    "best = fmin(lightgbm_factory, space_lgb, algo=algo, max_evals=20, pass_expr_memo_ctrl=None)\n",
    "print('The best params are:',best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best =  {'num_leaves': 7, 'max_depth': 12, 'colsample_bytree': 0.3273079033503896,\r\n",
    "'reg_alpha': 2.984483664182527, 'subsample': 0.3126121982112917,'reg_lambda': 0.2449221888407772,\r\n",
    "'learning_rate': 0.49692793913575733, 'subsample_freq': 3, 'n_estimators': 3854}\r\n",
    "best = argsDict_tranform_lgb(best)\r\n",
    "lgb = LGBMClassifier(max_depth=best['max_depth'],n_estimators=best['n_estimators'],\r\n",
    "                        learning_rate=best['learning_rate'],subsample=best['subsample'],\r\n",
    "                        num_leaves=best['num_leaves'],objective='binary',\r\n",
    "                        colsample_bytree=best['colsample_bytree'],reg_alpha=best['reg_alpha'],\r\n",
    "                        subsample_freq=best['subsample_freq'],reg_lambda=best['reg_lambda'],random_seed=66)\r\n",
    "\r\n",
    "print('feature percentile=%d' %i)\r\n",
    "trainx,predict_x = featureSelect(trainx,data.label[data.label != -1].values,predict_x)\r\n",
    "x_train,x_val,y_train,y_val = train_test_split(trainx,data.label[data.label != -1].values,\r\n",
    "train_size=0.8,random_state=6)\r\n",
    "# lgb\r\n",
    "lgb.fit(x_train,y_train,eval_set=[(x_train, y_train), (x_val, y_val)],\r\n",
    "eval_names=['train', 'val'],eval_metric=f1_metric,early_stopping_rounds=100,verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGB 参数调优\n",
    "# 定义参数空间\n",
    "space_xgb = {\n",
    "    'n_estimators':hp.randint('n_estimators',2000),\n",
    "    'max_depth':hp.randint('max_depth',10),\n",
    "    'subsample':hp.uniform('subsample',0.5,1),\n",
    "    'min_child_weight':hp.randint('min_child_weight',500),\n",
    "    'eta':hp.uniform('eta',0,0.4),\n",
    "    'colsample_bytree':hp.uniform('colsample_bytree',0.5,1),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.1, 3),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.1, 3)\n",
    "}\n",
    "\n",
    "def argsDict_tranform_xgb(argsDict, isPrint=False):\n",
    "    argsDict[\"max_depth\"] = argsDict[\"max_depth\"] + 6\n",
    "    argsDict[\"n_estimators\"] = argsDict['n_estimators'] + 500\n",
    "    if isPrint:\n",
    "        print(argsDict)\n",
    "    else:\n",
    "        pass\n",
    "    return argsDict\n",
    "\n",
    "def get_tranformer_score(tranformer):\n",
    "    model = tranformer\n",
    "    prediction = model.predict(x_val)\n",
    "    return -1*f1_score(y_val,prediction)\n",
    "    \n",
    "# 创建模型工场\n",
    "def xgb_factory(argsDict):\n",
    "    argsDict = argsDict_tranform_xgb(argsDict)\n",
    "    xgb = XGBClassifier(max_depth=argsDict['max_depth'],n_estimators=argsDict['n_estimators'],\n",
    "                        learning_rate=argsDict['eta'],subsample=argsDict['subsample'],\n",
    "                        colsample_bytree=argsDict['colsample_bytree'],reg_alpha=argsDict['reg_alpha'],\n",
    "                        reg_lambda=argsDict['reg_lambda'],random_seed=66,n_jobs=8)\n",
    "    xgb.fit(x_train,y_train,\n",
    "    eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    eval_metric=f1_xgb,\n",
    "    verbose = 50,\n",
    "    early_stopping_rounds=50)\n",
    "    return get_tranformer_score(xgb)\n",
    "algo = partial(tpe.suggest, n_startup_jobs=1)\n",
    "best_xgb = fmin(xgb_factory, space_xgb, algo=algo, max_evals=20, pass_expr_memo_ctrl=None)\n",
    "print('The best params are:',best_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_xgb = {'min_child_weight': 334, 'n_estimators': 69, 'max_depth': 2,\n",
    "            'eta': 0.12371645516780436, 'reg_alpha': 2.9121568142476426,\n",
    "            'subsample': 0.9430955192093492, 'reg_lambda': 1.4594989969245042,\n",
    "            'colsample_bytree': 0.9982012497911378}\n",
    "best_xgb = argsDict_tranform_xgb(best_xgb)\n",
    "xgb = XGBClassifier(max_depth=best_xgb['max_depth'],n_estimators=best_xgb['n_estimators'],\n",
    "                        learning_rate=best_xgb['eta'],subsample=best_xgb['subsample'],\n",
    "                        colsample_bytree=best_xgb['colsample_bytree'],reg_alpha=best_xgb['reg_alpha'],\n",
    "                        reg_lambda=best_xgb['reg_lambda'],random_seed=66,n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extra Tree\r\n",
    "from sklearn.ensemble import ExtraTreesClassifier\r\n",
    "param_grid = {'n_estimators':[300,600,1200]}\r\n",
    "model2 = ExtraTreesClassifier(verbose=10,n_jobs=8)\r\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle = True,random_state=7)\r\n",
    "gsv2 = GridSearchCV(model2,param_grid,scoring = 'f1',cv = kfold)\r\n",
    "print('fitting......')\r\n",
    "gsv2_result = gsv2.fit(x_train,y_train)\r\n",
    "print(\"Best of ETC: %f using %s\" % (gsv2_result.best_score_,gsv2_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators=1000,n_jobs=8,verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stacking Class\r\n",
    "class Ensemble(object):\r\n",
    "    def __init__(self, n_splits, stacker, base_models):\r\n",
    "        self.n_splits = n_splits\r\n",
    "        self.stacker = stacker\r\n",
    "        self.base_models = base_models\r\n",
    "\r\n",
    "    def fit_predict(self, X, y, T):\r\n",
    "        X = np.array(X)\r\n",
    "        y = np.array(y)\r\n",
    "        T = np.array(T)\r\n",
    "\r\n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, random_state=66).split(X, y))\r\n",
    "\r\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\r\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\r\n",
    "        \r\n",
    "        for i, clf in enumerate(self.base_models):\r\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\r\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\r\n",
    "                X_train = X[train_idx]\r\n",
    "                y_train = y[train_idx]\r\n",
    "                X_holdout = X[test_idx]\r\n",
    "                y_holdout = y[test_idx]\r\n",
    "                print (\"Fit Model %d fold %d\" % (i, j))\r\n",
    "                if str(clf)[:3].lower() == 'xgb':\r\n",
    "                    clf.fit(X_train, y_train,\r\n",
    "                    eval_set=[(X_train, y_train), (X_holdout, y_holdout)],\r\n",
    "                    eval_metric=f1_xgb,verbose = 50,early_stopping_rounds=50)\r\n",
    "                elif str(clf)[:3].lower == 'lgb':\r\n",
    "                    clf.fit(X_train,y_train,eval_set=[(X_train, y_train), (X_holdout, y_holdout)],\r\n",
    "                    eval_names=['train', 'val'],eval_metric=f1_metric,early_stopping_rounds=100,verbose=50)\r\n",
    "                else:\r\n",
    "                    clf.fit(X_train,y_train)\r\n",
    "                try:\r\n",
    "                    y_pred = clf.predict_proba(X_holdout)[:]\r\n",
    "                    S_train[test_idx, i] = y_pred[:,1]\r\n",
    "                except:\r\n",
    "                    y_pred = clf.predict(X_holdout)[:]\r\n",
    "                    S_train[test_idx, i] = y_pred\r\n",
    "                try:\r\n",
    "                    S_test_i[:, j] = clf.predict_proba(T)[:,1][:]\r\n",
    "                except:\r\n",
    "                    S_test_i[:, j] = clf.predict(T)[:]\r\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\r\n",
    "        self.stacker.fit(S_train, y)\r\n",
    "        res = self.stacker.predict(S_test)[:]\r\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "stack = Ensemble(n_splits=3,\n",
    "        stacker=LinearRegression(),\n",
    "        base_models=(xgb,etc,lgb))\n",
    "# y_val_pre = stack.fit_predict(x_train, y_train, x_val)\n",
    "# f1_metric(y_val,np.round(y_val_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = np.round(stack.fit_predict(trainx,data.label[data.label != -1].values, predict_x))\n",
    "submit = pd.DataFrame()\n",
    "submit['sid'] = data[data.label == -1]['sid']\n",
    "submit['label'] = result\n",
    "filename = 'submission'+ datetime.now().strftime('%m%d_%H%M')+'.csv'\n",
    "submit.to_csv(filename,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.5.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
